\documentclass[
12pt,								% Schriftgröße
DIV10,
a4paper,         		% Papierformat
oneside,						% einseitiges Dokument
%	titlepage,					% es wird eine Titelseite verwendet
parskip=half,				% Abstand zwischen Absätzen (halbe Zeile)
headings=normal,			% Größe der Überschriften verkleinern
listof=totoc,					% Verzeichnisse im Inhaltsverzeichnis aufführen
bibliography=totoc,						% Literaturverzeichnis im Inhaltsverzeichnis aufführen
index=totoc,						% Index im Inhaltsverzeichnis aufführen
%	caption=tableheading,	% Beschriftung von Tabellen unterhalb ausgeben
final								% Status des Dokuments (final/draft)
]{book}
\def\@makechapterhead#1{%
  \vspace*{50\p@}%
  {\parindent \z@ \raggedright \normalfont
    \ifnum \c@secnumdepth >\m@ne
      \if@mainmatter
        %\huge\bfseries \@chapapp\space \thechapter
        \Huge\bfseries \thechapter.\space%
        %\par\nobreak
        %\vskip 20\p@
      \fi
    \fi
    \interlinepenalty\@M
    \Huge \bfseries #1\par\nobreak
    \vskip 40\p@
  }}
%--------------------------------------------------------------------------------
% Ab hier werden Packages geladen
\usepackage{blindtext}
\usepackage{lmodern}
\usepackage{caption}
\usepackage[ngerman]{babel}
\usepackage[latin1]{inputenc}
\usepackage[T1]{fontenc}  
\usepackage{amsmath}
\usepackage{blindtext}
\usepackage{ulem} 
\usepackage[latin1]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[ngerman]{babel}
\usepackage{color}
\usepackage{graphicx}
\graphicspath{{./Bilder/}}
%\usepackage{subcaption}
\usepackage{cite}
\usepackage{float}
\usepackage{amsmath,amsfonts}
\usepackage{amssymb}
\usepackage{exscale}
\usepackage{subfig} 					
\usepackage{booktabs}
\usepackage{ulem}
\usepackage{setspace}

%für code
\usepackage{listings}
\usepackage{color}

\usepackage[a4paper,lmargin={25mm},rmargin={25mm},tmargin={25mm},bmargin= {25mm}]{geometry}
\addtolength{\footskip}{-0.8cm}		%Fussbereich 0.8cm höher, sodass die Seitennummierung höher ist
\usepackage{tabularx}
\usepackage{tabulary}
\newcolumntype{w}[1]{>{\raggedleft\hspace{0pt}}p{#1}}
\usepackage{eurosym}
\usepackage{siunitx}
\sisetup{exponent-product= \cdot ,output-decimal-marker = {,},detect-family,detect-display-math = true,per-mode = symbol-or-fraction}


\usepackage{lmodern}
\usepackage{fancyhdr} %Paket laden
\usepackage[format=hang]{caption}
%\setcapindent{0pt}




\usepackage{listings}
\usepackage{color}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
	backgroundcolor=\color{backcolour},   
	commentstyle=\color{codegreen},
	keywordstyle=\color{magenta},
	numberstyle=\tiny\color{codegray},
	stringstyle=\color{codepurple},
	stringstyle=\color{codepurple},
	basicstyle=\footnotesize,
	breakatwhitespace=false,         
	breaklines=true,                 
	captionpos=b,                    
	keepspaces=true,                 
	numbers=left,                    
	numbersep=5pt,                  
	showspaces=false,                
	showstringspaces=false,
	showtabs=false,                  
	tabsize=2
}


\lstset{numbers=left, numberstyle=\small, numbersep=8pt, frame = single, language=Python, framexleftmargin=1pt, style=mystyle}
%--------------------------------------------------------------------------------
% Hier wird die Titelseite bearbeitet. Im folgenden kommen die zu ändernden Daten
\newcommand{\nameOfTitel}{Projektdokumentation}
\newcommand{\untertitel}{DCAITI Projekt}	% wird nicht verwendet
\newcommand{\art}{Konzeption und Implementierung einer Augmented Reality basierten Applikation für historische Gebäude und Baustellen}
\newcommand{\autor}{Herbert Potechius und Linh Kästner}
\newcommand{\betreuer}{Konstantin Klipp}
\newcommand{\datum}{\today}				% hier Abgabedatum einfügen


%-----------------------------------------------------------------------------
\begin{document}
	
	% Ab hier wird die Titelseite zusammengesetzt. Es ist keine Änderung mehr notwendig!
	\thispagestyle{plain}
	\begin{titlepage}
		
		~\vspace{0.5cm} 
		\begin{center}
			\includegraphics[width = 0.24\textwidth]{tu-logo_rot}\hspace{2cm} 
			\includegraphics[width=0.5\textwidth]{fokus_Logo_930}\hspace{1cm}\\[1cm]
			\Large{TECHNISCHE UNIVERSITÄT BERLIN}\\
			Fakultät IV - Elektrotechnik und Informatik\\
			Fachgebiet DCAITI
		\end{center}
		
		\vspace{1.2cm}
		\begin{center}
			\LARGE{\textbf{\textsc{\nameOfTitel}}}\\
			\LARGE{\textbf{\untertitel}}\\[2ex]
			\Large{\textbf{\art}}\\[8ex]
			
			\normalsize
			\begin{tabular}{w{5.4cm}p{8cm}}\\
				vorgelegt von:	 & \quad \autor\\[1.2ex]
				Betreuer: & \quad \betreuer\\[1.2ex]
				%Labortermin: & \quad \labortermin\\[1.2ex]
				eingereicht am: &  \quad \datum\\[3ex]
			\end{tabular}
		\end{center}
		
	\end{titlepage}
	\newpage
	
	%-----------------------------------------------------------------------------
	\pagenumbering{roman}
	
	%-----------------------------------------------------------------------------
	% Eidesstattliche Versicherung der selbstständigen Arbeit
	\section*{Eidesstattliche Erklärung}
	Wir, \autor, versichern hiermit an Eides statt, dass wir unsere \textsc{\nameOfTitel} - \textit{\untertitel} mit dem Thema
	\begin{quote}
		\textit{\art}  
	\end{quote}
	selbständig und eigenhändig angefertigt und keine anderen als die angegebenen Quellen und Hilfsmittel benutzt haben.
	
	\bigskip
	\bigskip
	\bigskip
	
	Berlin, den \datum
	
	
	\rule[-0.2cm]{5cm}{0.5pt}
	
	\textsc{\autor} 
	
	\newpage
	%-----------------------------------------------------------------------------
	\pagestyle{fancy}
	\fancyhf{}
	\fancyhead[L]{\leftmark}
	\fancyhead[R]{Herbert Potechius, Linh Kästner} %Kopfzeile rechts
	\fancyfoot[LE,RO]{\thepage}
	\setlength{\headheight}{8pt}
	
	\renewcommand{\headrulewidth}{0.4pt}
	
	
	\renewcommand{\lstlistingname}{Code-Ausschnitt}% Listing -> Algorithm
	
	\tableofcontents % Inhaltsverzeichnis
	\newpage
	\pagenumbering{arabic}
	
	%---------------------------------------------------------------------
	% Ab hier fängt der eigenständige Teil an!
	
	\chapter{Einleitung und Motivation}
	\section{Einleitung}
	Der stetige Fortschritt im Bereich der Informationstechnik setze in den vergangenen Jahren in nahezu allen Bereichen der Gesellschaft innovative Möglichkeiten in Gang. 
	Das Konzept einer erweiterten Realität (Augmented Reality - AR) in der die reale Umgebung mittels virtuellen Instanzen erweitert wird, erlangt dabei, aufgrund seiner vielfältigen Einsatzmöglichkeiten, zunehmende Wichtigkeit. AR kann die Nutzerinteraktion erheblich verbessern und Aufgaben wie Wartung, Steuerung oder Kontrolle effizienter und einfacher gestalten. Durch die Visualisierung innerhalb der realen Umgebung trägt AR ferner zur Unterstützung des räumlichen Verständnisses bei und hat damit einhergehend positive Auswirkungen auf das Nutzererlebnis. Wesentliche Probleme beziehungsweise Herausforderung beim Einsatz von AR ist der Koordinatenabgleich zwischen Weltkoordinaten und dem verwendeten Darstellungsgerät - meist das Smartphone oder sogenannte Head Mounted Devices (HMD) wie die Microsoft Hololens um die Illusion zu schaffen, dass Daten und Hologramme sich innerhalb der echten Welt befinden. Dazu wurden in den vergangenen Jahren durch intensivierte Forschung eine Vielzahl an Methoden entwickelt auf welche im späteren Verlauf noch eingegangen wird. Weitere Herausforderungen siedeln sich im Bereich der Computervision zur intelligenten Erkennung und Auswertung der Pixeldaten, der Computergrafik sowie der Bildverarbeitung, beispielsweise das Rendering von Objekten in die Szene oder die korrekte Darstellung der Objekte, an. 
	
	\section{Zielstellung}
	Ziel dieses Projektes war es eine AR basierte Applikation zu implementieren, welche dem Nutzer erlaubt, innerhalb der realen Umgebung, virtuelle Instanzen zu platzieren. Konkret soll es sich dabei um Gebäude und Gegenstände handeln, um somit eine Erweiterung von Baustellen oder Ruinen durch jene virtuellen Instanzen zu erreichen und dabei unterstützend bei der Konstruktion, Planung und Design von neuen Gebäuden zu dienen. Außerdem kann, beispielsweise durch die Erweiterung von Ruinen und der Platzierung von historischen Objekten ein interessantes Nutzererlebnis ermöglicht werden. 
	


	
	
	
	
	
	
	
	\newpage
	\chapter{Theorie}
	
	Um ein optimales AR Erlebnis bieten zu können, müssen, wie bereits eingangs erwähnt, Aspekte aus verschiedenen Teildisziplinen miteinander verknüpft werden. Wichtige Disziplinen sind dabei die Computervision sowie die Computergrafik. Außerdem ist der Abgleich er Koordinatensysteme von essentieller Bedeutung. Um ein besseres Verständnis über die verwendete Theorie zu schaffen, wird im Folgenden auf die wichtigsten Aspekte und Ansatzpunkte dieser Problemstellungen eingegangen.
	

	\section{Paradigmen zur Detektion der Kamerapose im Raum}
	Ein Kernaspekt zur Ermöglichung eines angemessenen AR Erlebnisses ist der sogenannte Koordinatenabgleich der Weltkoordinaten mit denen der Kamera. Grundsätzlich geht es darum die Position des Nutzers und dessen Sichtfeld und Blickrichtung innerhalb der Weltkoordinaten zu lokalisieren. Praktisch entspricht dies der Orientierung und Position der Kamera des AR Geräts (Smartphone, HMD, o.ä.). Forschungen, welche z.T noch hochaktuell sind haben zahlreiche Algorithmen und Lösungsansätze hervorgebracht, wobei die wichtigsten Paradigmen im folgenden aufgelistet sind \cite{theory} 
	
	\begin{itemize}
		
	
	\item \textbf{Visuelle Erkennungsmethoden}. Hier wird die Position der Kamera anhand von visuellen Informationen geschätzt. Das System tätigt Rückschlüsse auf Position der Kamera anhand von Informationen, welche es visuell mitbekommt.
	 Nachteile dieser Methoden ergeben sich bei unbekannten Umgebungen, womit vorher erst genügend Daten gesammelt werden müssen um eine korrekte Poseschätzung zu erreichen. Der Markerbasierte Ansatz gehört zu dieser Art von Erkennung, welcher es durch vordefinierte Muster, jenes Problem löst. Dies wird im weiteren Verlauf detailliert erläutert. 
	 Weitere Methoden der visuellen Erkennung sind beispielsweise die Modellbasierten Erkennungsmethoden, welche einen Abgleich zwischen visuellen Daten mit vordefinierten Modellen (bspw. CAD Modellen) bewerkstelligen. 
	 
		\item \textbf{Sensorbasierte Erkennungsmethoden}
		Hier werden verschiedene, dem System verfügbare Sensordaten wie beispielsweise, Tiefendaten oder Lokalisierungsdaten bei der Positionsbestimmung verwendet. Dafür werden spezielle Geräte verwendet, welche diese zusätzlichen Informationen bereitstellen können, beispielsweise Gyroskope, GPS oder Tiefenkameras. Sensordaten können entweder die Position oder Orientierung des Nutzers wiedergeben. Die Kombination aus Position und Orientierung wird auch \textbf{Pose} genannt. 
		\item \textbf{hybride Ansätze} Hybride Ansätze vereinen oben genannte Methoden um eine optimierte/genauere Schätzung zu erreichen
		 Ein Ansatz beispielsweise die Kombination zwischen
		einem GPS Signal, welches die Position der Kamera vermitteln kann und visuellen Informationen, welche dann die Orientierung der Kamera festlegen. 


	\end{itemize}
	
	In der Praxis hat sich unter den oben vorgestellen Methoden der Markerbasierte Ansätz als Kompromiss zwischen Genauigkeit und Einfachheit in der Implementierung als weit verbreitet herausgestellt. Durch die vordefinierten Markerinformationen wird erheblich Zeit und Rechenleistung bei der Erkennung eingespart. Viele der führenden AR Frameworks nutzen die Markerbasierte Erkennung (ARToolkit, ARTag, Aruco). Die vordefinierten Informationen beispielsweise eine MarkerID zu welchem bestimmte Objekte eingeblendet werden sollen sind weitere Vorteile des Ansatzes. Nachteile sind, dass Marker für einige Anwendungsfälle nicht geeignet sind, beispielsweise bei Anwendungen in gefährlichen Gebieten in welchem dann auf die Sensorbasierten bzw der Kombination aus visuellen Informationen mit zusätzlichen Senrdaten gesetzt wird.
	
	Im Rahmen des Projektes wurde aufgrund obiger Erkenntnisse und der Empfehlung durch den Betreuer, der Markerbasierte Abgleich mittels Aruco genutzt, welche Teil der OpenCV Bibliothek ist. Auf diesen wird im folgenden detailliert eingegangen.
	
	
		

	\section{Die marker-basierte Detektion mit Aruco}
	Trotz der erwähnten Vorteile der Marker gibt es einige Aspekte zu beachten. Beispielsweise können ungünstige Lichtverhältnisse oder schlechter Kontrast dazu führen dass der Marker nicht erkannt wird. Aufgrund von Farbveränderungen, welche durch ungünstiges Licht auftreten können werden grundsätzlich nur schwarz-weiß Marker benutzt. Optimalerweise reichen 4 bekannte Punkte aus um die Pose des Markers zu erkennen. Daher eignen sich Quadratische Formen als Marker am besten. Es gibt zahlreiche bekannte Bibliotheken, welche diese Art von Markern verwendet. Eine davon ist die Aruco Bibliothek, welche teil des OpenCV Frameworks ist und in diesem Projekt verwendet wird. Aruco benutzt Schachbrettmuster um individuelle Marker zu erzeugen. Diese sind durch Ihre individuellen Muster mit einer ID versehen, was bspw. nützlich ist um bestimmte Objekte mit einem bestimmten Marker zu versehen. 
Für die Markererkennung und Identifikation nutzt Aruco spezielle Muster, welche dekodiert werden. Der Marker besteht aus einem schwarzen Rand sowie einem Muster im inneren, welches einer binären Matrix gleichkommt. Dabei bestimmt die Größe des Markers auch die Bitzahl der Innenmatrix. Entsprechend hat ein 4cm x 4 cm großer Marker eine 16 Bit Matrix. Auf diese Matrix wird im späteren Verlauf noch eingegangen. In Abbildung \ref{aruco} sind Beispiele dieser Marker zu sehen.
\begin{figure}[H]
		\centering
		\includegraphics[width= 0.7\textwidth]{aruco}
		\caption{Koordinatensysteme mit Freiheitsgraden \cite{opencv}}
		\label{aruco}
	\end{figure}
	
	
		
Im folgenden werden die einzelnen Schritte bei der Markererkennung besprochen \cite{opencv}.


\begin{itemize}
		
	
	\item Umwandlung des Bildes in ein Greyscale Bild, welches die Intensitätswerte beinhaltet. 
	 
		
		\item Erkennung der Kanten und Ecken im Bild und Bestimmung potentieller Markerkandidaten 
		\item Herausfilterung und Verbesserung 
		\item Dekodierung der Marker durch das Auswerten des Innenmusters des Markes. 
		\item Berechnung der Position und Orientierung des Markers (Posebestimmung)

	\end{itemize}	
	
	
	\subsubsection{Greyscaling und Tresholding}
	Aus dem aktuellen Bild der Kamera muss als erstes ein Grayscale Bild, also ein Bild aus Intensitätswerten bestimmt werden, da weitere Operationen auf diese Werte aufbauen. Um aus diesem Bild die Konturen zu erkennen, wird es 'getresholdet'.
	Ein für einen Beispielmarker generiertes Treshold Bild ist in Abb. zu sehen.
	
	\begin{figure}[H]
		\centering
		\includegraphics[width= 0.9\textwidth]{treshold}
		\caption{links: Originalbild, rechts: Treshold Bild \cite{theory}}
		\label{treshold}
	\end{figure}
	
	Der Treshold Ansatz erweisst sich als stabil gegen mögliche Helligkeitsschwankungen. Nachdem 'getresholdet' wurde liegt ein binäres Bild, bestehend aus Hintergrund und Objekten vor, aus denen dann die Konturen erkannt werden können. Dafür werden die Kanten aller potentiellen Marker im Bild markiert und geprüft ob genau 4 gerade Linien erkannt wurden \cite{theory}.
	Sind diese Voraussetzungen erfüllt, werden die Ecken des potentiellen Markers analysiert. 
	
	\subsubsection{Filterung und Verbesserung}
Nachdem Tresholding wurden Konturen, welche auf einen potentiellen Marker hindeuten erkannt. Jedoch sind nicht alle erkannten Konturen tatsächlich Marker. Daher ist der Filterungschritt ein essentieller Schritt um nur tatsächliche Marker zu berücksichtigen. Dabei werden alle jene Konturen herausgefiltert, welche offensichtlich keine Marker sind oder zu nah beieinander liegen. Dafür existieren in Aruco diverse Funktionen und Parameter um die Filterung individuell strikt zu gestalten. Wichtig ist es dabei die Markerparameter, wie Markergröße oder Maximale Distanz, welche zwei Marker zueinander besitzen dürfen vorher zu definieren um die Filterung effizient zu gestalten. 

	
	\subsubsection{Markerdekodierung und Identifikation}
	Nachdem die Markerkandidaten detektiert wurden kann die Detektierung des Markers beginnen um zu prüfen ob es sich tatsächlich um einen Aruco basierten Marker handelt.
	Dazu wird das Bitmuster im Inneren des Markers mittels dem sogenannten Otsu Algorithmus extrahiert um schwarze und weiße Bildpunkte zu differenzieren. Anschließend wird das Bild mittels eines Gitters aufgeteilt, welches erlaubt jene einzelnen schwarz-weiß Punkte exakt auszumachen. 
	Anschließend werden weiße und schwarze Bits gezählt. Dabei wird für jede Zelle nur die Mitte beachtet, um somit Störungen zu vermeiden, da es auch Zellen gibt in denen beide Farben existieren und somit nicht eindeutig ist um was für eine Zelle es sich handelt.
	Dies wird in Abbildung \ref{aruco3} deutlich.
	\begin{figure}[H]
		\centering
		\includegraphics[width= 0.7\textwidth]{aruco3}
		\caption{verbessertes Bitmuster im Gitter \cite{opencv}}
		\label{aruco3}
	\end{figure}
	
	Nachdem die Bits extrahiert und gezählt wurden, können diese mit dem Dictionary verglichen werden, in welchem sich alle vordefinierten Marker befinden.  
	
\newpage


\section{Der Koordinatenabgleich}
Ist der Marker erkannt, kann der eigentliche Koordinatenabgleich zwischen der Kamera, welche den Marker detektiert hat und dem Marker selbst stattfinden. Dies wird ist im Fachjargon auch unter dem Perspective-n-Point Problem bekannt.
Zuvor muss jedoch erst eine Kamerakalibrierung stattfinden, in welchem die Kameramatrix, bestehend aus intrinsischen Parametern der Kamera, wie Brennweite und Verzerrungselemente gefunden werden, welche in einer Kameramatrix resultiert. OpenCV und Aruco bieten dazu einige Funktionen an mit denen die Kamera kalibriert werden kann.
Hauptziel des Koordinatenabgleiches ist die Bestimmung der Position der Kamera im Raum basierend auf der Markerposition. 
In Abbildung \ref{koord1} ist eine Übersicht der beiden Koordinatensysteme veranschaulicht. 
	\begin{figure}[H]
		\centering
		\includegraphics[width= 0.8\textwidth]{koord}
		\caption{Koordinatensysteme mit Freiheitsgraden \cite{theory}}
		\label{koord1}
	\end{figure}
	

	Position und Orientierung der Kamera erzeugen somit 6 Freiheitsgrade. Die Koordinaten (x,y,z) beschreiben dabei die Translation und die Winkel ($\alpha , \beta , \gamma$) die Rotation. Um diese 6 Freiheitsgrade zu finden, werden die zuvor detektierten Punkte des Markers verwendet. Durch die Kenntnis über die Entfernung des Markers zur Kamera (durch die vorherige Markerdetektierung), wird eine Transformationsmatrix $T_{cam}$ zur, die einen Koordinatenabgleich zwischen Kamera und Marker ermöglicht. Diese ist definiert als 
	
	
	\begin{align*}
	x = T \cdot X
	\end{align*}
	Oder umgeschrieben
		\begin{figure}[H]
		\centering
		\includegraphics[width= 0.3\textwidth]{eq}
		
		\label{eq}
	\end{figure}
	Wobei X für einen Punkt in Weltkoordinaten, x für dessen Projektion in Kamerakoordinaten und T die Transformationsmatrix ist.
	Diese Matrix beinhaltet die Translation $(t_x,t_y,t_z)$ und Rotation $(r_1,..,r_9)$ der Kamera in Relation zum Marker basierend auf den gefundenen Punkten des detektierten Markers und wird daher auch als Posematrix bezeichnet. 
Durch Anwenden dieser Transformation geschieht der Abgleich zwischen Welt und Kamera. Verbildlicht bedeutet das, dass eine virtuelle Kamera auf den Marker gesetzt wird, was nun ein positionsgenaues Rendern der Objekte auf oder in Relation zur Markerpose ermöglicht. 	
	Dafür muss eine weitere Transformation mit der Kameramatrix getätigt werden, welche dann die 3-dimensionalen Bildpunkte in die 2D Szene des sichtbaren Bildes der Kamera (Image Plane) liefert. Der Prozess ist in Abbildung  \ref{koord3} veranschaulicht. 

	
	
	\begin{figure}[H]
		\centering
		\includegraphics[width= 0.60\textwidth]{koord3}
		\caption{Koordinatensysteme mit Freiheitsgraden \cite{theory}}
		\label{koord3}
	\end{figure}
	Soll das Objekt in Relation zum Marker platziert werden, muss eine weitere Transformationsmatrix $T_{object}$ angegeben werden in welcher Translation und Rotation definiert sind (Abbildung \ref{koord4}) .
	\begin{figure}[H]
		\centering
		\includegraphics[width= 0.60\textwidth]{koord4}
		\caption{Koordinatensysteme mit Freiheitsgraden \cite{theory}}
		\label{koord4}
	\end{figure}
	
		Um eine erfolgreiche Detektion von Markern zu verifizieren, wird in Aruco nach erfolgreicher Detektion meist ein gerendertes Koordinatensystem auf den Marker gezeigt,wie dies in Abbildung \ref{aruco2} zu sehen ist. 
	
	
		\begin{figure}[H]
		\centering
		\includegraphics[width= 0.4\textwidth]{aruco2}
		\caption{Erfolgreiche Markererkennung und Objektplatzierung mit Aruco \cite{opencv} }
		\label{aruco2}
	\end{figure}
%-----------------------------------------------------------------------------
%-----------------------------------------------------------------------------
%-----------------------------------------------------------------------------
%-----------------------------------------------------------------------------
\section{Chromaticity-Bild}
Sind Bilder invariant gegenüber Helligkeitsveränderungen, kann ein und das selbe Objekt bei unterschiedlichen Helligkeiten besser identifiziert werden. Um diese Invarianz zu erreichen, wird das RGB-Bild in den rgb-Farbraum überführt. Die Transformation erfolgt nach folgenden Gleichungen:
\begin{align*}
	r=\dfrac{R}{R+G+B}~~~~~~~~~g=\dfrac{G}{R+G+B}~~~~~~~~~b=\dfrac{B}{R+G+B}
\end{align*}
Die Farbe des Pixels definiert sich nicht mehr durch die Intensität der einzelnen Farbkanäle, sondern durch das Verhältnis der Farbkanäle untereinander. Somit bilden z.B. die beiden Farbwerte (255,0,0) und (150,0,0) aus dem RGB-Farbraum auf den selben Farbwert (1,0,0) im rgb-Farbraum ab.


\begin{figure}[H]
		\centering
		\includegraphics[width= 0.75\textwidth]{chroma1}
		\caption{Vergleich: Originalaufnahme mit Chromaticity-Bild}
		\label{chroma}
	\end{figure}


Damit dieses Verfahren zur Markererkennung verwendet werden kann, müssen die Marker farblich angepasst werden, sodass Unterschiede in der Farbart erkennbar sind. Dieser Ansatz wurde für das Preprocessing genutzt, um Marker auch bei schlechten Lichtverhältnissen zu erkennen.


\begin{figure}[H]
		\centering
		\includegraphics[width= 0.65\textwidth]{chroma2}
		\caption{Anpassung der Markerfarben}
		\label{anpassung}
	\end{figure}


%-----------------------------------------------------------------------------
%-----------------------------------------------------------------------------
%-----------------------------------------------------------------------------
%-----------------------------------------------------------------------------
	\chapter{Setup}
	
	Zur Realisierung der Applikation müssen zuvor besprochene Aspekte umgesetzt werden. Dafür gibt es eine Vielzahl an vorhandenen Frameworks und Bibliotheken, welche in der Realisierung der spezifischen Teilaspekte verwendet werden. Im Folgenden werden diese zusammen mit dem Gesamtsetup vorgestellt.
	
	
	\section{Verwendete Frameworks und Software}
	\textbf{OpenCV}
 OpenCV ist eine Bibliothek welche sich mit einer Vielzahl an Aspekten rund um die Bilderkennung und der maschinellen Verarbeitung beschäftigt. Anwendungen reichen von der Gesichtserkennung über Tracking bis hin zu neuronalen Netzen \cite{wikicv}. In dem Projekt wird OpenCV zur Markererkennung genutzt.\\ 
\textbf{OpenGL} OpenGL ist ebenfalls eine frei verfügbare Bibliothek, welche Funktionen zur Verwendung mit 3D Objekten bereitstellt. Funktionen umfassen beispielsweise die Darstellung der Objekte in Echtzeit \cite{wikigl}, dem Einsatz von Spezialeffekten und vielen weiteren Funktionen zur grafischen Bearbeitung. Für das Projekt von Bedeutung ist das Darstellen der 3D Objekte. Dazu werden Funktionen für das Rendern dieser Objekte genutzt. OpenCV und OpenGL lassen sich durch Schnittstellen problemlos miteinander verwenden. \\
\textbf{Android Studio}
Zur Entwicklung der Applikation wurde Android Studio benutzt. Somit kommen als Zielgruppe ausschließlich Android Geräte in Frage. Dies hat den Grund einer offeneren Plattform und der leichteren Einbindung von externen Bibliotheken wie OpenCV oder OpenGl. Android Studio ist eine Entwicklungsumgebung für die Entwicklung von Applikationen für das Android Betriebssystem. Es beinhaltet diverse Möglichkeiten zur Einbindung von externen Bibliotheken und Paketen. Um obige Bibliotheken innerhalb der Entwicklungsumgebung nutzen zu können müssen diese zuvor in das Projekt geladen werden. Dazu wurden die Schritte aus \cite{int} und \cite{int2} befolgt. Für eine detaillierte Beschreibung der Einbindung sei auf diese Quellen verwiesen.

	

%-----------------------------------------------------------------------------
%-----------------------------------------------------------------------------
%-----------------------------------------------------------------------------
%-----------------------------------------------------------------------------	
	\chapter{Implementierung}
	Im folgenden Kapitel wird die Implementierung der Applikation erklärt. Dies soll dazu dienen, den Aufbau der App sowie die Funktionsweise der einzelnen Funktionen nachzuvollziehen. Die Implementierung richtet sich hauptsächlich an der vorher besprochenen Theorie der Bildverarbeitung, Markererkennung und Objektpositionierung. Geschrieben wurde die Applikation in anfangs in Java. Aus performancetechnischen Gründen wurden später wichtige Funktionen in C++ umgeschrieben. 
Der komplette Quellcode mit Erläuterungen kann auf Anfrage gestellt werden. In Abbildung \ref{flow1} ist das Flussdiagramm verbildlicht. 
	
			\begin{figure}[H]
		\centering
		\includegraphics[width= 1.0\textwidth]{flow_chart}
		\caption{Flussdiagramm der Arbeitsschritte}
		\label{flow1}
	\end{figure}
	
	Beim Start der Applikation wird nach der Bilderfassung ein Preprocessing durchgeführt, da, wie bereits in der Theorie erwähnt bei ungünstigen Lichtverhältnissen oder Farben, eine fehlerfreie Detektion verhindern. Wurde der Marker dann erkannt, wird die in ihm definierte ID extrahiert und die Markerposition berechnet um entsprechend das 3D Modell auf den Marker zu rendern. Außerdem wurden auch Objekte in Relation zum Marker dargestellt, wobei hier die eine zusätzliche Transformationsmatrix $T_{object}$ angewandt wurde. Die Matrix muss, abhängig der gewünschten Position des Objektes zum Marker, mit entsprechenden Werten für Translation und Rotation versehen werden. Die genaue Anwendung wird im Kapitel Ergebnisse und Performance beschrieben.
	Im folgenden sollen die wichtigsten Funktionen der einzelnen Teilblöcke erläutert werden.
%-----------------------------------------------------------------------------
%-----------------------------------------------------------------------------
\newpage
\section{Bilderfassung}
Durch Instanziierung der von Android bereitgestellten Klasse \textit{JavaCameraView} kann auf die Aufnahmen der Kameras zugegriffen werden. Diese werden für eine spätere Analyse gespeichert.
	
\begin{table}[H]
	\centering
	\begin{tabularx}{\textwidth}{|p{0.47\textwidth}|p{0.475\textwidth}|}
		\hline
			Funktion & Beschreibung \\ 
			\hline
			\textbf{init\_camera}() & Initialisiert die Rück-Kamera des Smartphones mit einer vordefinierten Auflösung. \\
			\hline
			\textbf{onCameraFrame}(CvCameraViewFrame  inputFrame) & Wird pro Frame aufgerufen und liefert das Kamerabild als \textit{CvCameraViewFrame}\\
			\hline
\end{tabularx}
\caption{Funktionen zur Bilderfassung}
\end{table} 


%-----------------------------------------------------------------------------
%-----------------------------------------------------------------------------
\section{Preprocessing}
Das als \textit{CvCameraViewFrame} vorliegende Bild wird zur weiteren Bearbeitung in eine Matrix gespeichert. Aus performancetechnischen Gründen wird das Preprocessing in C++ ausgeführt. Dieser Schritt sorgt dafür, dass der Marker unter verschiedensten Lichtverhältnissen erkannt wird. Für das Preprocessing wurden verschiedene Ansätze ausprobiert. Einer der Ansätze beinhaltete die Idee den Kontrast und die Helligkeit iterativ zu erhöhen bis der Marker gefunden wird. Aufgrund der Anzahl der Werte, die pro Frame ausprobiert werden müssen, sinkt jedoch performance stark, was allerdings mit einem leistungsfähigem Smartphone ausgeglichen werden kann. Als alternativer Ansatz wurde ebenfalls die Iteration über das gesamte Bild realisiert. Dabei wird adaptives thresholding auf das Bild angewandt. Das bedeutet, dass das Bild regional mit verschiedenen thresholds binarisiert wird. Die Anzahl der weißen Pixel liefert dann einen ungefähres Maß für die Helligkeit des Bildes, anhand dessen dann Helligkeit und Kontrast erhöht werden.
Ein dritter Ansatz bringt die Frames in ein Lichtveränderunginvariantes Format(Chromaticity-Bild), wobei hier die theoretischen Aspekte aus Kapitel 2.4 genutzt wurden. Auch dies resultiert in einem nicht immer optimalem Ergebnis. 

\begin{table}[H]
	\centering
	\begin{tabularx}{\textwidth}{|p{0.47\textwidth}|p{0.475\textwidth}|}
		\hline
			Funktion & Beschreibung \\ 
			\hline
			\textbf{analyzeMarker}(...) & Ruft die C++-Funktion auf, die die eigentliche Preprocessing-Funktionalität beinhaltet\\
			\hline
			\textbf{preprocessing}(Mat mRgb, Mat mChrom) & Die Matrix \textit{mRgb} wird in ein Chromaticity-Bild transformiert und in \textit{mChrom} gespeichert. \\
			\hline
\end{tabularx}
\caption{Funktionen zum Preprocessing}
\end{table} 

\newpage

%-----------------------------------------------------------------------------
%-----------------------------------------------------------------------------
\section{Markeranalyse}
Die Markeranalyse enthält die Schritte \textit{Markererkennung}, \textit{Marker\_ID extrahieren} und \textit{Markerposition berechnen}. Diese werden auch aus performancetechnischen Gründen in C++ ausgeführt. Alle drei Schritte werden von der Funktion \textit{detect}(...) aus der Aruco-Library durchgeführt.

\begin{table}[H]
	\centering
	\begin{tabularx}{\textwidth}{|p{0.47\textwidth}|p{0.475\textwidth}|}
		\hline
			Funktion & Beschreibung \\ 
			\hline
			\textbf{detect}(Mat input, vector<Marker> detectedMarkers, CameraParameters camParams,
                    float markerSize) & Sucht innerhalb des Bildes \textit{input} nach Markern und speichert diese in \textit{detectedMarkers}. Anhand der Kameraparameter \textit{camParams} und der Markergröße \textit{markerSize} kann die Orientierung des Markers bezüglich der Kamera berechnet werden. Wie in der Theorie beschrieben wird hier die Marker-ID bestimmt.\\
			\hline
\end{tabularx}
\caption{Funktionen zur Markeranalyse}
\end{table} 

%-----------------------------------------------------------------------------
%-----------------------------------------------------------------------------
\section{Rendering}
Durch die Initialisierung des Renderers wird pro Frame die Orientierung des Markers verwendet um das 3D-Modell zu positionieren.

\begin{table}[H]
	\centering
	\begin{tabularx}{\textwidth}{|p{0.47\textwidth}|p{0.475\textwidth}|}
		\hline
			Funktion & Beschreibung \\ 
			\hline
			\textbf{initOpenGL}()& Initialisiert den Renderer und sorgt dafür, dass die onDrawFrame-Methode pro Frame ausgeführt wird.\\
			\hline
			\textbf{onDrawFrame(GL10 gl)} & Wendet die Transformationsmatrizen des Markers auf das 3D-Modell an und projiziert dieses auf die Bildebene.\\
			\hline
\end{tabularx}
\caption{Funktionen zum Rendern des 3D-Modells}
\end{table} 

\section{Generierung von 3D Objekten}
Für die virtuelle Erweiterung von Baustellen ung Ruinen, müssen entsprechende Modelle entweder schon vorhanden sein oder generiert werden. Im Projekt wurde ein Teil der verwendeten 3D Objekte selbst generiert. Der Prozess zur Erstellung dieser Objekte wird im folgenden näher erläutert um auch in Zukunft selbst beliebige Objekte erzeugen zu können. 


\begin{figure}[H]
		\centering
		\includegraphics[width= \textwidth]{3d}
		\caption{Schritte zur Generierung des 3D Objektes}
		\label{3d}
	\end{figure}
	
	Zur Generierung eines 3D-Modells eines realen Objektes wird das Photogrammetrie-Programm \textit{Agisoft Photoscan} verwendet. Dieses verlangt als Input mehrere Fotos des zu rekonstruierenden Objektes aus verschiedenen Perspektiven. Durch den Perspektivenunterschied zwischen den Bildern, kann ihre relative Position untereinander berechnet werden und somit pro Bildpunkt ein Raumpunkt bestimmt werden. Die Gesamtheit dieser Raumpunkte ergibt eine \textit{Point Cloud}. Diese wird im nächsten Schritt zur \textit{Dense Point Cloud} verdichtet und über Triangulationsverfahren in ein \textit{Mesh} transformiert.  Die nun orientierten Kameras liefern die Textur für das Mesh. Das \textit{Textured Mesh} wird in Form einer OBJ-Datei gespeichert, sodass es von der App gelesen werden kann. \\
	Als Objekt wurde exemplarisch eine Statue erstellt, wie sie beispielsweise als altes Artefakt in Ruinen dargestellt werden kann.
	Da das Ziel der Applikation eine Erweiterung von Baustellen und alten Ruinen durch Einblendung von virtuellen Objekten ist, wurden außerdem 3D Objekte von Gebäuden benutzt. In Abbildung \ref{objects} sind die verwendeten 3D Objekte gezeigt. Für komplexe Gebäude wurden frei verfügbare 3D Objekte aus dem Internet verwendet. Grundsätzlich kann somit jedes beliebige 3D Objekt in der Applikation dargestellt werden.

\begin{figure}[H]
		\centering
		\includegraphics[width= \textwidth]{Objekts}
		\caption{Verwendete Objekte \cite{3d}}
		\label{objects}
	\end{figure}
	
	\newpage
	
\section{Aufbau der Applikation}
Das Design der Applikation wurde, im Sinne einer einfachen Bedienung, so übersichtlich wie möglich gehalten. Um die Applikation auf Funktionalitäten zu testen und das Debugging einfacher zu gestalten wird eine Version der Applikation benutzt wie sie in Abbildung \ref{app2} zu erkennen ist. Beim Starten der Applikation werden die Schritte aus dem Flussdiagramm in Abbildung \ref{flow1} gestartet. Wird der Marker erkannt werden entsprechend seiner ID dann die Objekte in der Welt angezeigt. Sowohl zu Testzwecken als auch für eine verbesserte Nutzerinteraktion, wurden diverse Buttons integriert. Die Buttons Greyscale (GREY), Threshhold (THR) dienen dazu dem Entwickler die einzelnen Bildstufen aufzuzeigen um Verbesserungen beispielsweise im Preprocessing vorzunehmen. Der Button ORIG zeigt das Original Bild, das von der Kamera aufgenommen wird, ohne jegliche Bearbeitung. Mit EDG, AXIS werden die Kanten des Markers und ein Koordinatensystem des Markers angezeigt. Dies kann zur Verifizierung einer erfolgreichen Detektion genutzt werden. Weiter ist es dem Nutzer möglich das Objekt durch den Button OBJ auszublenden um die Originalszene zu bekommen. 
Mit den Balken Contrast und Brightness auf der rechen Seiten der Applikation können Kontrast und Helligkeit des Bildes manuell eingestellt werden. Diese Schritte übernimmt in der endgültigen Applikation das Preprocessing. 
\begin{figure}[H]
		\centering
		\includegraphics[width= 0.6\textwidth]{app_debug}
		\caption{Applikation ohne Debugging Buttons}
		\label{app2}
	\end{figure}
Als zusätzliche Option ein Button zum Einblenden von objektspezifischen Informationen implementiert. Dieser blendet einen selbst definierten Text in die Szene ein. Dies kann beispielsweise bei zukünftigen Bauprojekten oder Ruinen von Bedeutung sein um Informationen zum Objekt wie Baujahr, Kosten oder Bedeutung wiederzugeben. Um die Applikation übersichtlicher zu gestalten werden die Testbuttons entfernt, was in der Applikation in Abbildung \ref{app2} resultiert. 
\begin{figure}[H]
		\centering
		\includegraphics[width= 0.6\textwidth]{app_reduced}
		\caption{Applikation ohne Debugging Buttons}
		\label{app2}
	\end{figure}
	Hier ist nun nur noch die Kameraszene zu erkennen sowie ein Button zur Einblendung objektspezifischer Informationen. 

\section{Performance} 
Im Folgenden sind die Ergebnisse der resultierenden Applikation aufgezeigt. Die Applikation zeigt beim Scannen des Markers die Objekte entweder genau auf dem Marker platziert oder in einer bestimmten Relation zu ihm. Entsprechend werden somit unterschiedliche Anwendungsfälle berücksichtigt.
	
	\begin{figure}[H]
		\centering
		\includegraphics[width= \textwidth]{test1}
		\caption{links: Erweiterung einer Ruine/Baustelle, rechts: Einblendung einer Statuet }
		\label{objects}
	\end{figure}
Zu erkennen ist die Erweiterung der Umgebung mit den 3D Objekten mit realen Abmessungen. Abbildung \ref{objects} links zeigt die relative Positionierung des Objektes. Dafür muss, wie bereits in der Theorie besprochen, eine entsprechende Transformationsmatrix erstellt werden, welche Werte für Translation und Rotation vom Marker beinhaltet. Somit können die Objekte in beliebiger Relation zum Marker platziert werden. Wird keine zusätzliche Transformationsmatrix angegeben, wird das Objekt direkt auf den Marker platziert, wie in Abbildung \ref{objects} rechts zu erkennen ist.\\
Im folgenden sind wichtige Performance Werte aufgeführt.
	\begin{table}[H]
	\centering
	\begin{tabularx}{\textwidth}{|p{0.47\textwidth}|p{0.475\textwidth}|}
		\hline
			Eigenschaft & Wert \\ 
			\hline
			\textbf{Frame Rate}& 13 FPS\\
			\hline
			\textbf{Auflösung} & 320x240 px \\
			\hline
\end{tabularx}
\caption{App Performance}
\end{table} 
Durch die Auflösung von 320x240 px kann eine angemessene Darstellung des Objektes ohne wesentliche Störeffekte erzielt werden. Die Frame Rate liegt dabei im Schnitt bei 13 FPS, was bedingt ist durch die geringe Leistungsfähigkeit des Smartphones. Eine wesentlich bessere Framerate ließ sich mit einem Samsung Galaxy S5 erzielen. Zum Vergleich sind im Folgenden wichtige Eigenschaften der Smartphone Hardware aufgelistet. Ein Test der Applikation mit aktuelleren Smartphones wird entsprechend noch bessere Ergebnisse liefern. 
\begin{table}[H]
	\centering
	\begin{tabularx}{\textwidth}{|p{0.47\textwidth}|p{0.475\textwidth}|}
		\hline
			Eigenschaft & Wert \\ 
			\hline
			\textbf{CPU-Frequenz}& 1300 MHz\\
			\hline
			\textbf{CPU-Kerne}& 8\\
			\hline
			\textbf{GPU-Frequenz} & 600 MHz \\
			\hline
			\textbf{GPU-Kerne} & 1 \\
			\hline
			\textbf{Arbeitsspeicher} & 3 GByte \\
			\hline
\end{tabularx}
\caption{Smartphone Hardware}
\end{table} 


Der größte Teil der verbrauchten Rechenleistung wird der Markererkennung und dem Objekt rendering zugeschrieben. Beides wurde anfangs mittels Java implementiert. Nach der Umstellung auf C++ konnte eine verbesserte Performance erreicht werden. Auch das Preprocessing in C++ trägt zu einer verbesserten Performance bei. Eine gezielte Verbesserung der Performance lässt sich in der Optimierung der genannten Aspekte bewerkstelligen.
Zusammenfassend lässt sich sagen, dass die Platzierung von beliebigen 3D Objekten im Raum jedoch weitestgehend ohne größere Störungen funktioniert und das noch vorhandene, leichte Ruckeln mit einem leistungsfähigerem Smartphone ausgeglichen werden kann. 

\newpage
\chapter{Zusammenfassung und Ausblick}
Insgesamt wurden die Konzepte der Theorie verstanden und im Projekt erfolgreich angewandt.
Damit konnten die anfangs definierten Ziele für das Projekt alle erreicht werden.
Dies umfasst die erfolgreiche Implementierung einer Markerbasierten Schätzung der Pose der Kamera innerhalb der Weltkoordinaten durch die Markerdetektion. Damit verbunden wurden verschiedene Ansätze des Preprocessings ausprobiert um den Marker auch bei schlechten Lichtverhältnissen zu erkennen. Da diese Ansätze nicht immer optimale Ergebnisse liefern, ist eine Optimierung dieser Ansätze ein wichtiger nächster Schritt. 
Basierend auf den Informationen der Markerdetektion, gelang weiterhin die Kombination mit OpenGL zur positionsgenauen Platzierung von 3D Objekten innerhalb der Welt. Somit konnte das Grundgerüst für eine AR basierte Applikation zur virtuellen Erweiterungen von vorher bekannten Szenerien wie Gebäuden, Baustellen oder Ruinen implementiert werden. Die verwendeten 3D Modelle sind dabei mit lebensechten Abmessungen versehen und bieten so dem Nutzer eine erhebliche Verbesserung der Vorstellung über die Gesamtszene, was vor allem in der Konstruktion oder Planung von neuen Gebäuden essentiell ist. Weiterhin wurden zusätzliche Funktionen wie das Einblenden von objektspezifischen Informationen oder Bildstufen in Form von Buttons integriert, um dem Nutzer eine Interaktionsmöglichkeit mit der Applikation zu schaffen und dem Entwickler bei der Entwicklung von neuen Funktionen oder der Optimierung beispielsweise des Preprocessings zu unterstützen.
Ferner wurde auch ein Ansatz zur Erstellung von 3D Objekten mittels Pointclouds vorgestellt. Damit ist es möglich aus Bildern bekannter realer Objekte, 3D Objekte zu erzeugen, welche dann beispielsweise in  einer anderen Umgebung visualisiert werden können. 

\subsection{Ausblick}
Die existierende App bietet viel Freiraum für zukünftige Erweiterungen. Eine wichtige Erweiterung ist das Füllen der internen Datenbank mit neuen 3D Modellen. Dazu können verschiedene MarkerIds verschiedenen Objekten zugeordnet und der vorgestellte Prozess zur Erstellung von 3D Objekten genutzt werden.
Ein weiterer Punkt ist eine erweiterte Benutzerinteraktion durch Buttons. Eine Möglichkeit ist die Einblendung von Informationen über verfügbare Marker in der Stadt. Dafür kann die Integration einer Map, beispielsweise mittels der Google Maps API für Android, in die App integrieren. 
Ein weiterer wichtiger Aspekt ist die Optimierung der Preprocessing Algorithmen. Diese liefern teilweise nicht immer optimale Ergebnisse. Auch weitere Algorithmen können probiert werden ,welche eine noch robustere Markererkennung bei schlechten Verhältnissen bieten. 


%-----------------------------------------------------------------
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%literatur
% Abschließend sind die Quellen von Bildern und Zitaten nicht zu vergessen.

\newpage
\newpage
\bibliographystyle{plain}
\begin{thebibliography}{}   
	
		\bibitem{opencv} 
	opencv.org
	
	\bibitem{theory} 
	Theory and applications
of marker-based
augmented reality, Sanni Siltanen
	
	\bibitem{wikicv}
	https://de.wikipedia.org/wiki/OpenCV
	
		\bibitem{wikigl}
	https://de.wikipedia.org/wiki/OpenGL
	
	\bibitem{int}
	https://blog.codeonion.com/2015/11/25/creating-a-new-opencv-project-in-android-studio/
	
	\bibitem{int2}
	https://medium.com/@sukritipaul005/a-beginners-guide-to-installing-opencv-android-in-android-studio-ea46a7b4f2d3
	
	\bibitem{chroma}
	https://en.wikipedia.org/wiki/Rg\_chromaticity
	
	\bibitem{3d}
	https://free3d.com/
	
	\bibitem{io}          
	Ablaufpläne\\
	\newblock{https://www.draw.}
	io/
	%\bibitem{Informationen}
	%Claire Grube, Rainer Wahnsinn und Kurt Zschluß 
	%\newblock {\it Praktikumsbeschreibung Kurbelwelleninnenrandbeleuchtung,}
	%\newblock TU Berlin, 2014.
\end{thebibliography}

\end{document}